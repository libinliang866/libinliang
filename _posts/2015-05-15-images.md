---
layout: post
title:  Deep neural network with infinite width
date: 2021-12-03 21:01:00
description: Deep neural network with infinite width
tags: DNNGP
categories: Deep learning
---

To study the infinite width of neural network is of interest because of two main reasons. The first one is that the neural networks applied in the real application usually have a large width so they may share similar properties as neural networks with inifinite width. The other reason is that the neural networks with inifinite width usually have explicit properties that is easy to study. Here we try to introduce the result about the initial distribution of deep neural network with inifinite width from Jaehoon Lee etc. In this result, the initial output of the deep neural network with inifinite width will become a Gaussian Process with a kernel having a compositional structure. 

## Structure of Deep Neural Network

Given input $$x\in \mathbb{R}^d$$, the deep neural network with $$L$$ hidden layers can be expressed as the following.

$$
z_i^l(x)=b_i^l+\sum_{j=1}^{N_l}W_{ij}^lx_{j}^l(x),~~~~~ x_j^l(x)=\phi(z_j^{l-1}(x))~~~~~~~ l=1,...,L
$$

where $$\phi(\cdot)$$ is the non-linear activation such as $$tanh$$ or $ReLU$.

## Deep Neural Network with Infinite width 

We can first derive the distribution of the output for one layer and then extend to multiple layers. 

### Single Layer Case

Now we consider the deep neural network with only 1-layer. Given the bias $$b_i^0\sim_{i.i.d}N(0,\sigma_b^2)$$ and weight $W_{ij}^0 \sim_{i.i.d} N(0,\sigma_w^2)$, then we could have 

$$
z_i^0(x)\sim_{i.i.d} N(0, \sigma_b^2+\sigma_w^2 ||x||^2)~~~~ i=1,...,N_1
$$

And we have $$x_i^1 = \phi(z_i^0(x))$$ are i.i.d.(Independent and identical distributed).

