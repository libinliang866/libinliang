---
layout: post
title:  Tangent Kernel of Deep Neural Network
date:   2022-02-26 16:40:16
description: Deep Neural Network Tangent Kernel
tags: NTK
categories: deep learning
---

Given the neural network with the struture as below:

{% include figure.html path="assets/img/ntk_1.png" class="img-fluid rounded z-depth-1" %}

If we consider the inner product of the gradient for different input $$x$$ and $$x^\prime$$, given the width of the neural network is infiite, the inner product will be deterministic kernel dependending on the depth $$L$$ and the non-linearity activation. 

{% include figure.html path="assets/img/ntk_2.png" class="img-fluid rounded z-depth-1" %}

{% include figure.html path="assets/img/ntk_3.png" class="img-fluid rounded z-depth-1" %}

With the gradient, we can write down the linearization of the neural network.

{% include figure.html path="assets/img/ntk_4.png" class="img-fluid rounded z-depth-1" %}

And then we can derive the expression of the linearization of the neural network during training. 

{% include figure.html path="assets/img/ntk_5.png" class="img-fluid rounded z-depth-1" %}

Given new testing point $$x\in X_T$$, we can derive the distribution of the output of the trained linearization neural network as below

{% include figure.html path="assets/img/ntk_6.png" class="img-fluid rounded z-depth-1" %}

Results from paper Wide Neural Networks of Any Depth Evolve asLinear Models Under Gradient Descent claim that with the width of the neural network large enough, the neural network will be close to the linearization counterparts.
